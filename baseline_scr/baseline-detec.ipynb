{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!rm -rf /kaggle/working/PDLPR-algorithm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/giankev/PDLPR-algorithm.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IMPORT","metadata":{}},{"cell_type":"code","source":"# standard library\nimport os\nimport sys\nimport math\nimport time\nimport shutil\nimport tarfile\nimport warnings\nfrom pathlib import Path\n\n# utility\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport time\n\n#PyTorch & torchvision\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms as T\nfrom torchvision.ops import box_iou\nimport torchvision.transforms as T\n\n#Albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\n#Custom repo modules \nrepo_path = \"/kaggle/working/PDLPR-algorithm/baseline_scr/detection\"\nsys.path.insert(0, repo_path)\nfrom model import LPDetectorFPN\nfrom trainer import set_seed, train, train_one_epoch, evaluate, ciou_loss, cxcywh_to_xyxy\nsys.path.remove(repo_path)\n\nwarnings.filterwarnings(\"ignore\")\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:37:10.407564Z","iopub.execute_input":"2025-07-20T23:37:10.407862Z","iopub.status.idle":"2025-07-20T23:37:14.662057Z","shell.execute_reply.started":"2025-07-20T23:37:10.407839Z","shell.execute_reply":"2025-07-20T23:37:14.661461Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## SETUP ENVIRONMENT","metadata":{}},{"cell_type":"code","source":"#downloading 50 imgs for train and 8k for test\n!gdown --folder https://drive.google.com/drive/folders/143HxhUrqkFIdfCzZQ3dA4Mqt8cjARCxx?usp=sharing -O datasets\n#https://drive.google.com/drive/u/1/folders/1Qirh0lsjdsroLHEmJDtS6sVXPQKalW6j","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extracting the .tar archive.\ndef extract_tar_archive(archive_path, destination_path):\n\n    print(f\"Extracting the tar archive in:{archive_path}\")\n    with tarfile.open(archive_path, \"r\") as tar:\n        tar.extractall(path=destination_path)\n        \n    print(f\"Archive extracted in: {destination_path}\")\n\n#delete the .tar archive which now is useless.\ndef delete_tar_archive(path_tar_archive):\n    \n    if os.path.exists(path_tar_archive):\n        shutil.rmtree(path_tar_archive)\n        print(f\"Folder eliminated: {path_tar_archive}\")\n    else:\n        print(f\"Folder not found: {path_tar_archive}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"archive_path_train = \"/kaggle/working/datasets/ccpd_train.tar\"\narchive_path_test = \"/kaggle/working/datasets/ccpd_test.tar\"\nextract_path = \"/kaggle/working/\"\n\n#when extracting the files, is important to eliminate the .tar archive which now occupy /kaggle/working space.\nextract_tar_archive(archive_path_train, extract_path)\nextract_tar_archive(archive_path_test, extract_path)\ndelete_tar_archive(\"/kaggle/working/datasets/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FUNCTION","metadata":{}},{"cell_type":"code","source":"#extracting the metadata from each img in this format (image_path,x1_bbox,y1_bbox,x2_bbox,y2_bbox)\ndef split_bbox(bbox_str):\n    \"extracting x1,y1,x2,y2, ex. '283___502_511___591'  â†’  ['283','502','511','591']\"\n    tokens = []\n    for seg in bbox_str.split(\"___\"):\n        tokens.extend(seg.split(\"_\"))\n    if len(tokens) == 4 and all(t.isdigit() for t in tokens):\n        return map(int, tokens)\n    return (None,)*4\n\ndef count_parameters(model):\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters:     {total:,}\")\n    print(f\"Trainable parameters: {trainable:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:37:22.242901Z","iopub.execute_input":"2025-07-20T23:37:22.243606Z","iopub.status.idle":"2025-07-20T23:37:22.249085Z","shell.execute_reply.started":"2025-07-20T23:37:22.243579Z","shell.execute_reply":"2025-07-20T23:37:22.248324Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def preprocess_resize_bbox(df: pd.DataFrame,\n                           out_dir: str = \"preproc224\",\n                           img_size: int = 224,\n                           save_as_pt: bool = False,\n                           quality: int = 95) -> pd.DataFrame:\n    \"\"\"\n    Resize images and bounding boxes, saving the output and returning an updated DataFrame.\n\n     df         : DataFrame containing columns: image_path, x1_bbox, y1_bbox, x2_bbox, y2_bbox\n     out_dir    : output folder to save resized images or tensors\n     img_size   : target size (square, e.g. 224)\n     save_as_pt : if True, saves images as .pt tensors; else as JPEG\n     quality    : JPEG quality (only used if save_as_pt is False)\n    \"\"\"\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    records = []\n    to_tensor = torch.from_numpy  # shortcut for converting numpy to tensor\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preâ€‘resize\"):\n        # Load image and convert to RGB\n        img = Image.open(row.image_path).convert(\"RGB\")\n        w0, h0 = img.size  # original dimensions\n\n        # Resize image to target size (e.g., 224x224) using bilinear interpolation\n        img = img.resize((img_size, img_size), Image.BILINEAR)\n\n        # Scale bounding box coordinates to the new image size\n        sx, sy = img_size / w0, img_size / h0\n        x1 = row.x1_bbox * sx\n        y1 = row.y1_bbox * sy\n        x2 = row.x2_bbox * sx\n        y2 = row.y2_bbox * sy\n\n        # Get filename without extension\n        stem = Path(row.image_path).stem\n\n        if save_as_pt:\n            # Convert image to tensor and normalize to [0,1]\n            tensor = to_tensor(np.array(img)).permute(2, 0, 1).float() / 255\n            path_out = out_dir / f\"{stem}.pt\"\n            torch.save(tensor, path_out)\n        else:\n            # Save image as JPEG with given quality\n            path_out = out_dir / f\"{stem}.jpg\"\n            img.save(path_out, format=\"JPEG\", quality=quality, optimize=True)\n\n        # Store updated info (new image path and scaled bbox) in the new DataFrame\n        records.append({\n            \"image_path\": str(path_out),\n            \"x1_bbox\": x1, \"y1_bbox\": y1,\n            \"x2_bbox\": x2, \"y2_bbox\": y2,\n            **{c: row[c] for c in df.columns if c not in (\n               \"image_path\", \"x1_bbox\", \"y1_bbox\", \"x2_bbox\", \"y2_bbox\")}\n        })\n\n    return pd.DataFrame.from_records(records)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:37:24.595930Z","iopub.execute_input":"2025-07-20T23:37:24.596409Z","iopub.status.idle":"2025-07-20T23:37:24.604340Z","shell.execute_reply.started":"2025-07-20T23:37:24.596384Z","shell.execute_reply":"2025-07-20T23:37:24.603691Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"folder = \"/kaggle/working/ccpd_subset_base/train\"\nrows   = []\n\nfor fname in os.listdir(folder):\n    if not fname.endswith(\".jpg\"): continue\n\n    parts = fname[:-4].split(\"-\")           \n    if len(parts) < 6:\n        continue #the ccpd file name is wrong           \n\n    x1,y1,x2,y2 = split_bbox(parts[2])          \n    \n    rows.append({\n        \"image_path\": os.path.join(folder, fname),\n        \"x1_bbox\": x1, \"y1_bbox\": y1,\n        \"x2_bbox\": x2, \"y2_bbox\": y2\n    })\n\ndf = pd.DataFrame(rows)\nprint(\"Rows number:\", len(df))         \nprint(\"Columns numner:\", df.shape[1])\nprint(\"Shape:\", df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:37:30.502021Z","iopub.execute_input":"2025-07-20T23:37:30.502309Z","iopub.status.idle":"2025-07-20T23:37:30.801682Z","shell.execute_reply.started":"2025-07-20T23:37:30.502287Z","shell.execute_reply":"2025-07-20T23:37:30.800874Z"}},"outputs":[{"name":"stdout","text":"Rows number: 50000\nColumns numner: 5\nShape: (50000, 5)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                          image_path  x1_bbox  y1_bbox  \\\n0  /kaggle/working/ccpd_subset_base/train/0254310...      182      424   \n1  /kaggle/working/ccpd_subset_base/train/0188362...      220      544   \n2  /kaggle/working/ccpd_subset_base/train/0263146...      211      483   \n3  /kaggle/working/ccpd_subset_base/train/0320761...      187      520   \n4  /kaggle/working/ccpd_subset_base/train/0393247...      147      463   \n\n   x2_bbox  y2_bbox  \n0      500      530  \n1      456      637  \n2      460      600  \n3      478      639  \n4      509      573  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>x1_bbox</th>\n      <th>y1_bbox</th>\n      <th>x2_bbox</th>\n      <th>y2_bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/ccpd_subset_base/train/0254310...</td>\n      <td>182</td>\n      <td>424</td>\n      <td>500</td>\n      <td>530</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/ccpd_subset_base/train/0188362...</td>\n      <td>220</td>\n      <td>544</td>\n      <td>456</td>\n      <td>637</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/ccpd_subset_base/train/0263146...</td>\n      <td>211</td>\n      <td>483</td>\n      <td>460</td>\n      <td>600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/ccpd_subset_base/train/0320761...</td>\n      <td>187</td>\n      <td>520</td>\n      <td>478</td>\n      <td>639</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/ccpd_subset_base/train/0393247...</td>\n      <td>147</td>\n      <td>463</td>\n      <td>509</td>\n      <td>573</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## TRAIN PHASE","metadata":{}},{"cell_type":"code","source":"class PlateDatasetFastAug(Dataset):\n    def __init__(self, df, augment=True, img_size=224):\n        self.df = df.reset_index(drop=True)\n        self.augment = augment\n        self.img_size = img_size\n\n        if augment:\n            self.pipeline = A.Compose([\n                # Blurring\n                A.OneOf([\n                    A.GaussianBlur(blur_limit=3, p=0.3),\n                    A.MotionBlur(blur_limit=5, p=0.2),\n                ], p=0.6),\n                \n                # Brightness & contrast variation\n                A.OneOf([\n                    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n                    A.RandomBrightnessContrast(brightness_limit=(-0.6, -0.3), contrast_limit=0.2, p=0.6),\n                ], p=0.5),\n                \n                A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n                A.ColorJitter(0.2, 0.2, 0.2, 0.1, p=0.4),\n                \n                # Random crop or zoom-in\n                A.OneOf([\n                    A.Compose([\n                        A.LongestMaxSize(max_size=int(img_size * 0.6)),\n                        A.PadIfNeeded(img_size, img_size,\n                                      border_mode=cv2.BORDER_REPLICATE, value=(0, 0, 0)),\n                    ], p=0.5),\n                    A.RandomResizedCrop(size=(img_size, img_size), scale=(0.8, 1), ratio=(1.0, 1.0)),\n                ], p=0.35),\n\n                A.Perspective(scale=(0.03, 0.06), p=0.4),\n\n                # CLAHE or shadow simulation\n                A.OneOf([\n                    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n                    A.RandomShadow(num_shadows_lower=1, num_shadows_upper=2,\n                                   shadow_dimension=4, shadow_roi=(0, 0.4, 1, 1), p=0.5),\n                ], p=0.3),\n\n                # Downscale â†’ simulates low resolution\n                A.OneOf([\n                    A.Downscale(scale_min=0.3, scale_max=0.5, interpolation=cv2.INTER_LINEAR, p=1.0)\n                ], p=0.25),\n\n                ToTensorV2()\n            ], bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"labels\"]))\n        \n        else:\n            self.pipeline = A.Compose([\n                ToTensorV2()\n            ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # Load and convert image to RGB\n        img = cv2.cvtColor(cv2.imread(row.image_path), cv2.COLOR_BGR2RGB)\n\n        # Normalize bounding box to [0, 1]\n        x1, y1, x2, y2 = row.x1_bbox, row.y1_bbox, row.x2_bbox, row.y2_bbox\n        cx = (x1 + x2) / 2 / self.img_size\n        cy = (y1 + y2) / 2 / self.img_size\n        w  = (x2 - x1) / self.img_size\n        h  = (y2 - y1) / self.img_size\n        bbox_yolo = [cx, cy, w, h]\n\n        # Apply augmentation (or just transform)\n        transformed = self.pipeline(\n            image=img,\n            bboxes=[bbox_yolo],\n            labels=[0]\n        )\n\n        img_tensor = transformed[\"image\"].float() / 255.0\n        bbox = torch.tensor(transformed[\"bboxes\"][0], dtype=torch.float32)\n\n        return img_tensor, bbox","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the dataset for training phase.\ndf_train, df_val = train_test_split(df, test_size=0.02, shuffle=True, random_state=42)\n\nprint(f\"Train set: {len(df_train)} img\")\nprint(f\"Val set:   {len(df_val)} img\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Prepocess imgs into 224x224\ndf_train_224 = preprocess_resize_bbox(df_train, \"train224\", img_size=224, save_as_pt=False)\ndf_val_224   = preprocess_resize_bbox(df_val,   \"val224\",   img_size=224, save_as_pt=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = PlateDatasetFastAug(df_train_224, augment=True)\nval_ds   = PlateDatasetFastAug(df_val_224, augment=False)\n\ndl_train = DataLoader(train_ds, batch_size=128, shuffle=True,\n                      num_workers=4, pin_memory=True, persistent_workers=True)\ndl_val   = DataLoader(val_ds, batch_size=128, shuffle=False,\n                      num_workers=4, pin_memory=True, persistent_workers=True)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = LPDetectorFPN()      \ncount_parameters(model)\nmodel  = train(model, dl_train, dl_val,\n               epochs=20, lr=1e-4, device=device)\n\ntorch.save(model.state_dict(), \"/kaggle/working/lp2_detectorr.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TEST","metadata":{}},{"cell_type":"code","source":"root   = Path(\"/kaggle/working/ccpd_test\")   \nrows   = []\n\nfor jpg in root.rglob(\"*.jpg\"):\n    fname = jpg.name\n    parts = fname[:-4].split(\"-\")            \n    if len(parts) < 6:\n        continue  #wrong name file                         \n\n    try:\n        x1, y1, x2, y2 = split_bbox(parts[2])\n    except Exception as e:\n        print(\"skip\", jpg, e)\n        continue\n\n    rows.append({\n        \"subset\": jpg.parent.name,    \n        \"image_path\": str(jpg),\n        \"x1_bbox\": x1, \"y1_bbox\": y1,\n        \"x2_bbox\": x2, \"y2_bbox\": y2,\n    })\n\ndf = pd.DataFrame(rows)\n\nprint(\"Rows:\",   len(df))\nprint(\"Cols:\",   df.shape[1])\nprint(\"Shape:\",  df.shape)\ndisplay(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:38:08.532359Z","iopub.execute_input":"2025-07-20T23:38:08.532678Z","iopub.status.idle":"2025-07-20T23:38:08.638566Z","shell.execute_reply.started":"2025-07-20T23:38:08.532623Z","shell.execute_reply":"2025-07-20T23:38:08.637786Z"}},"outputs":[{"name":"stdout","text":"Rows: 8000\nCols: 6\nShape: (8000, 6)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    subset                                         image_path  x1_bbox  \\\n0  weather  /kaggle/working/ccpd_test/weather/0304-9_19-23...      230   \n1  weather  /kaggle/working/ccpd_test/weather/0295-4_1-253...      253   \n2  weather  /kaggle/working/ccpd_test/weather/0322-12_12-3...      304   \n3  weather  /kaggle/working/ccpd_test/weather/0455-18_17-4...      438   \n4  weather  /kaggle/working/ccpd_test/weather/0140-0_0-134...      134   \n\n   y1_bbox  x2_bbox  y2_bbox  \n0      484      457      596  \n1      428      490      532  \n2      384      502      520  \n3      508      659      680  \n4      387      326      448  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subset</th>\n      <th>image_path</th>\n      <th>x1_bbox</th>\n      <th>y1_bbox</th>\n      <th>x2_bbox</th>\n      <th>y2_bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>weather</td>\n      <td>/kaggle/working/ccpd_test/weather/0304-9_19-23...</td>\n      <td>230</td>\n      <td>484</td>\n      <td>457</td>\n      <td>596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>weather</td>\n      <td>/kaggle/working/ccpd_test/weather/0295-4_1-253...</td>\n      <td>253</td>\n      <td>428</td>\n      <td>490</td>\n      <td>532</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>weather</td>\n      <td>/kaggle/working/ccpd_test/weather/0322-12_12-3...</td>\n      <td>304</td>\n      <td>384</td>\n      <td>502</td>\n      <td>520</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>weather</td>\n      <td>/kaggle/working/ccpd_test/weather/0455-18_17-4...</td>\n      <td>438</td>\n      <td>508</td>\n      <td>659</td>\n      <td>680</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>weather</td>\n      <td>/kaggle/working/ccpd_test/weather/0140-0_0-134...</td>\n      <td>134</td>\n      <td>387</td>\n      <td>326</td>\n      <td>448</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# parameters setup\nIOU_THR  = 0.7            \nIMG_SIZE = 224            \ndevice   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#model init\n#otherwise you can use the weights from the previous training phase \nweights_github = \"/kaggle/working/PDLPR-algorithm/baseline_scr/detection/detec_weights.pt\" \ndetector = LPDetectorFPN().to(device)\ndetector.load_state_dict(torch.load(weights_github, map_location=device))\ndetector.eval()\n\n#utils\ntfm = T.Compose([\n    T.ToPILImage(),\n    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BILINEAR),\n    T.ToTensor()\n])\n\ndef bbox_iou(pred, tgt, eps=1e-7):\n    px1, py1 = pred[:, 0] - pred[:, 2] / 2, pred[:, 1] - pred[:, 3] / 2\n    px2, py2 = pred[:, 0] + pred[:, 2] / 2, pred[:, 1] + pred[:, 3] / 2\n    tx1, ty1 = tgt[:, 0] - tgt[:, 2] / 2, tgt[:, 1] - tgt[:, 3] / 2\n    tx2, ty2 = tgt[:, 0] + tgt[:, 2] / 2, tgt[:, 1] + tgt[:, 3] / 2\n\n    inter_w = (torch.min(px2, tx2) - torch.max(px1, tx1)).clamp(min=0)\n    inter_h = (torch.min(py2, ty2) - torch.max(py1, ty1)).clamp(min=0)\n    inter   = inter_w * inter_h\n    area_p  = (px2 - px1) * (py2 - py1)\n    area_t  = (tx2 - tx1) * (ty2 - ty1)\n    union   = area_p + area_t - inter + eps\n    return inter / union\n\nloss_fn = torch.nn.SmoothL1Loss(reduction=\"mean\")\n\n#evaluation subset\n@torch.no_grad()\ndef eval_subset(df_sub, name):\n    iou_list, loss_list = [], []\n    correct, t_forward, n_imgs = 0, 0.0, 0\n\n    for _, row in tqdm(df_sub.iterrows(), total=len(df_sub),\n                       desc=f\"{name:10}\", leave=False):\n\n        img_bgr = cv2.imread(row.image_path);  n_imgs += 1\n        if img_bgr is None: continue\n        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n        h0, w0  = img_rgb.shape[:2]\n\n        # preprocessing + forward (timed)\n        t0 = time.time()\n        img_t = tfm(img_rgb).unsqueeze(0).to(device)\n        pred  = detector(img_t)[0].cpu()              # cxcywh âˆˆ [0,1]\n        t_forward += (time.time() - t0)\n\n        # ground truth scale to IMGSIZE\n        sx, sy = IMG_SIZE / w0, IMG_SIZE / h0\n        x1, y1, x2, y2 = row.x1_bbox, row.y1_bbox, row.x2_bbox, row.y2_bbox\n        x1_r, y1_r, x2_r, y2_r = x1*sx, y1*sy, x2*sx, y2*sy\n        tgt = torch.tensor([(x1_r + x2_r) / (2*IMG_SIZE),\n                            (y1_r + y2_r) / (2*IMG_SIZE),\n                            (x2_r - x1_r) / IMG_SIZE,\n                            (y2_r - y1_r) / IMG_SIZE])\n\n        #metrics\n        iou  = bbox_iou(pred.unsqueeze(0), tgt.unsqueeze(0)).item()\n        loss = loss_fn(pred, tgt).item()\n        iou_list.append(iou);  loss_list.append(loss)\n        if iou >= IOU_THR: correct += 1\n\n    acc   = correct / n_imgs\n    m_iou = np.mean(iou_list)\n    m_l1  = np.mean(loss_list)\n    fps   = n_imgs / t_forward if t_forward else 0.0\n\n    print(f\"CCPD_{name:<9} | Acc@{IOU_THR}: {acc:.4f} | IoU: {m_iou:.3f} \"\n          f\"| L1: {m_l1:.4f} | img: {n_imgs} | FPS: {fps:.1f}\")\n    return n_imgs, acc, m_iou, m_l1, t_forward\n\n#subset loop\ndf[\"subset\"] = df.image_path.apply(lambda p: Path(p).parts[-2])\n\ng_imgs = g_acc = g_iou = g_l1 = total_time = 0.0\nfor sub in sorted(df.subset.unique()):\n    n, acc, miou, mloss, t = eval_subset(df[df.subset == sub], sub)\n    g_imgs += n;  g_acc += acc*n;  g_iou += miou*n;  g_l1 += mloss*n;  total_time += t\n\nif g_imgs:\n    print(f\"\\nðŸ”¹ GLOBAL | Acc@{IOU_THR}: {g_acc/g_imgs:.4f} \"\n          f\"| IoU: {g_iou/g_imgs:.3f} \"\n          f\"| L1: {g_l1/g_imgs:.4f} \"\n          f\"| FPS: {g_imgs/total_time:.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:38:13.988820Z","iopub.execute_input":"2025-07-20T23:38:13.989552Z","iopub.status.idle":"2025-07-20T23:40:10.320825Z","shell.execute_reply.started":"2025-07-20T23:38:13.989517Z","shell.execute_reply":"2025-07-20T23:40:10.319976Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"base      :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_base      | Acc@0.7: 0.9820 | IoU: 0.863 | L1: 0.0001 | img: 1000 | FPS: 87.9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"blur      :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_blur      | Acc@0.7: 0.8310 | IoU: 0.794 | L1: 0.0001 | img: 1000 | FPS: 94.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"challenge :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_challenge | Acc@0.7: 0.8630 | IoU: 0.798 | L1: 0.0001 | img: 1000 | FPS: 96.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"db        :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_db        | Acc@0.7: 0.7470 | IoU: 0.765 | L1: 0.0003 | img: 1000 | FPS: 93.6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"fn        :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_fn        | Acc@0.7: 0.7400 | IoU: 0.758 | L1: 0.0006 | img: 1000 | FPS: 94.8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"rotate    :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_rotate    | Acc@0.7: 0.9340 | IoU: 0.810 | L1: 0.0002 | img: 1000 | FPS: 96.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tilt      :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_tilt      | Acc@0.7: 0.8170 | IoU: 0.780 | L1: 0.0003 | img: 1000 | FPS: 99.6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"weather   :   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"CCPD_weather   | Acc@0.7: 0.9850 | IoU: 0.867 | L1: 0.0001 | img: 1000 | FPS: 96.0\n\nðŸ”¹ GLOBAL | Acc@0.7: 0.8624 | IoU: 0.804 | L1: 0.0002 | FPS: 94.7\n","output_type":"stream"}],"execution_count":6}]}